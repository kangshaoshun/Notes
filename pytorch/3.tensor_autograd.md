## Tensor
Tensor是pytorch中重要的数据结构，可以认为是一个高维数组，它可以是一个数（标量）,一维数组（向量），二维数组（矩阵）以及更高维的数组。Tensor和Numpy的ndarray类似，但是Tensor可以使用gpu进行加速。
```@python
from __future__ import print_function
import torch as t

#initialize
x = t.Tensor(5,3) #构建5*3矩阵，只是分配了空间，并没有初始化
x = t.rand(5, 3) #使用[0, 1]均匀分布随机初始化二维数组

#shape
x.size()	
x.size(0)	#行数
x.size(1)	#列数


#add
y = t.rand(5,3)
x + y	#第一种加法
t.add(x, y)	#第二种加法
*函数名后面带下划线_的函数会修改调用add_的Tensor本身，例如x.add_(y)会改变x值*
res = t.Tensor(5, 3)	#第三种加法，指定输出
t.add(x, y, out=res)


#transform
x[:1] #Tensor的选取操作和numpy类似，tensor还支持很多操作，包括数学运算，线性代数等
a = t.ones(5)	# tensor
b = a.numpy()	# tensor -> numpy
import numpy as np
a = np.ones(5)
b = t.from_numpy(a)	#numpy -> tensor
*tensor和numpy对象共享内存，所以他们之间的转换很快，几乎不会消耗什么资源*

#GPU
if t.cuda.is_available():	#tensor可以通过.cuda方法转为GPU的tensor,从而加速
	x = x.cuda()
	y = y.cuda()
	x + y
```


## Autograd 自动微分
深度学习的算法本质上是通过反向传播求导数，pytorch的autograd模块实现了此功能，在Tensor上的所有操作，Autograd都能为它自动提供微分，避免手动计算导数的复杂过程。

autograd.Variable是Autograd中的核心类，它简单封装了Tensor,并支持几乎所有的Tensor操作，Tensor在被封装为Variable之后，可以调用它的.backward实现反向传播，自动计算所有梯度。

Variable主要包含三个属性：
	data:保存Variable所包含的Tensor
	grad:保存data对应的梯度，grad也是个Variable
	grad_fn:指向一个Function对象，这个Function用来反向传播计算输入的梯度
```python
from torch.autograd import Variable

#initialize
x = Variable(t.ones(2, 2), requires_grad=True)

y = x.sum()	#y's value is 4
y.grad_fn	#<torch.autograd.function.SumBackward object at 0x7fc13f551af8>
y.backward()	#反向传播，计算梯度
#grad在反向传播过程中是累加的，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需要把梯度清零
x.grad.data_zero_()
```


## 神经网络
Autograd实现了反向传播功能，但是直接用来写深度学习的代码在很多情况下还是稍微复杂，torch.nn是专门为神经网络设计的模块化接口。nn构建与Autograd之上，可以用来定义和运行神经网络。nn.Module是nn中最重要的类，可以把它卡成是一个网络的封装，包含网络各层定义以及forward方法，调用forward(input)方法，可返回前向传播的结果。看例子LeNet:
```python
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
	def __init__(self):
		super(Net, self).__init__()	#nn.Module子类的函数必须在构造函数中执行父类的构造函数
		self.conv1 = nn.Conv2d(1, 6, 5)	#卷积层‘1’表示输入图片为单通道，'6'表示输出通道数，'5'表示卷积核为5*5
		self.conv2 = nn.Conv2d(6, 16, 5)
		self.fc1 = nn.Linear(16*5*5, 120)	#全连接层
		self.fc2 = nn.Linear(120, 84)
		self.fc3 = nn.Linear(84, 10)
	
	def forward(self, x):
		x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))	#卷积 -> 激活 -> 池化
		x = F.max_pool2d(F.relu(self.conv2(x)), 2)
		x = x.view(x.size()[0], -1)
		x = F.relu(self.fc1(x))
		x = F.relu(self.fc2(x))
		x = self.fc3(x)
		return x
net = Net()

只要在nn.Module的子类中定义了forward函数，backward函数就会自动被实现，在forward函数中可使用任何Variable支持的函数

网络的可学习参数通过net.parameters()返回，net.named_parameters可同时返回可学习的参数及名称
for name, parameters in net.named_parameters():
	print name,":",parameters.size()

ps:forward函数的输入和输出都是Variable，只有Variable才具有自动求导功能，而Tensor是没有的，所以在输入时，需要把Tensor封装成Variable.

需要注意的是torch.nn只支持mini-batches,不支持一次只输入一个样本，即一次必须是一个batch.
```
### 损失函数
	nn实现了神经网络中大多的损失函数，例如nn.MSELoss用来计算均方误差，nn.CrossEntropyLoss用来计算交叉熵损失。
	
### 优化器
在反向传播计算完成所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：
	weight = weight - learning_rate * gradient

torch.optim 中实现了深度学习中绝大多数的优化方法，例如RMSProp, Adam, SGD等
```python
import torch.optim as optim
optimizer = optim.SGD(net.parameters(), lr=0.01)
optimizer.zero_grad()
output = net(input)

loss = criterion(output, target)

loss.backward()	#反向传播
optimizer.step()#更新参数
```

### 数据加载与数据预处理
	在深度学习中数据加载及预处理是非常复杂的，pytorch提供了一些可以极大简化和加快数据处理流程的工具，同时，对于常用的数据集，pytorch也提供了封装好的接口供用户快速调用，这些数据集主要保存在torchvision中

